---
title: "r-exercise-3"
format: html
editor: visual
---

```{r}

########################################################
#                     Exercise 3.1
#
#         Classification: Alternative Techniques
#
########################################################

### Install packages ###

if(!require(pacman))
  install.packages("pacman")

pacman::p_load(
  C50,                # C5.0 Decision Trees and Rule-Based Models
  caret,              # Classification and Regression Training
  e1071,              # Misc Functions of the Department of Statistics (e1071), TU Wien
  keras,              # R Interface to 'Keras'
  kernlab,            # Kernel-Based Machine Learning Lab
  lattice,            # Trellis Graphics for R
  MASS,               # Support Functions and Datasets for Venables and Ripley's MASS
  mlbench,            # Machine Learning Benchmark Problems
  nnet,               # Feedforward Neural Networks and Multinomial Log-Linear Models
  palmerpenguins,     # Palmer Archipelago (Antarctica) Penguin Data
  party,              # A Laboratory for Recursive Partytioning
  partykit,           # A Toolkit for Recursive Partytioning
  randomForest,       # Breiman and Cutler's Random Forests for Classification and Regression
  FSelector,          # This will bring in "RWeka & also randomForest, et al." 
  rJava,              # I think this is needed for RWeka so I'll add it here
  rpart,              # Recursive partitioning models
  RWeka,              # R/Weka Interface
  scales,             # Scale Functions for Visualization
  tidymodels,         # Tidy machine learning framework
  tidyverse,          # Tidy data wrangling and visualization
  xgboost             # Extreme Gradient Boosting
)

### Install packages: Show fewer digits ###

options(digits=3)

### Training and Test Data ###

data(Zoo, package="mlbench")
Zoo <- as.data.frame(Zoo)
Zoo |> glimpse()
#Zoo

### 80% Training Data ###

set.seed(123)  # for reproducibility
inTrain <- createDataPartition(y = Zoo$type, p = .8)[[1]] #we're using this function from the caret library to split data into "test/training" partitions.
Zoo_train <- dplyr::slice(Zoo, inTrain)
Zoo_test <- dplyr::slice(Zoo, -inTrain)
#Zoo_train
#Zoo_test

### Fitting Different Classification Models to the Training Data ###
### 
### Creating an index for our training data. This "organization" is how we'll compare the models later and find the best fit.

train_index <- createFolds(Zoo_train$type, k = 10)

### Conditional Inference Tree (Decision Tree) ###

ctreeFit <- Zoo_train |> train(type ~ .,
  method = "ctree",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
ctreeFit

plot(ctreeFit$finalModel)

### C 4.5 Decision Tree ###

### "J48" is the C4.5 decision tree algorithm to predict the class label of new instances.

C45Fit <- Zoo_train |> train(type ~ .,
  method = "J48",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
C45Fit

C45Fit$finalModel

### K-Nearest Neighbors ###

knnFit <- Zoo_train |> train(type ~ .,
  method = "knn",
  data = _,
  preProcess = "scale",
    tuneLength = 5,
  tuneGrid=data.frame(k = 1:10),
    trControl = trainControl(method = "cv", indexOut = train_index))
knnFit

knnFit$finalModel

### PART (Rule-based classifier) ###

rulesFit <- Zoo_train |> train(type ~ .,
  method = "PART",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index))
rulesFit

rulesFit$finalModel

### Linear Support Vector Machines ###

svmFit <- Zoo_train |> train(type ~.,
  method = "svmLinear",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
svmFit

svmFit$finalModel

### Random Forest ###
###
### 1 of 2 Ensemble methods we'll use (RF & Boosting)

randomForestFit <- Zoo_train |> train(type ~ .,
  method = "rf",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index))
randomForestFit

randomForestFit$finalModel

### Gradient Boosted Decision Trees (xgboost) ###

xgboostFit <- Zoo_train |> train(type ~ .,
  method = "xgbTree",
  data = _,
  tuneLength = 5,
  trControl = trainControl(method = "cv", indexOut = train_index),
  tuneGrid = expand.grid(
    nrounds = 20,
    max_depth = 3,
    colsample_bytree = .6,
    eta = 0.1,
    gamma=0,
    min_child_weight = 1,
    subsample = .5
  ))
xgboostFit

xgboostFit$finalModel

### Artificial Neural Network ###

nnetFit <- Zoo_train |> train(type ~ .,
  method = "nnet",
  data = _,
    tuneLength = 5,
    trControl = trainControl(method = "cv", indexOut = train_index),
  trace = FALSE)
nnetFit

nnetFit$finalModel

### Comparing Models ###

resamps <- resamples(list(
  ctree = ctreeFit,
  C45 = C45Fit,
  SVM = svmFit,
  KNN = knnFit,
  rules = rulesFit,
  randomForest = randomForestFit,
  xgboost = xgboostFit,
  NeuralNet = nnetFit
    ))
resamps

summary(resamps)

library(lattice)
bwplot(resamps, layout = c(3, 1))

difs <- diff(resamps)
difs

summary(difs)

### Applying the Chosen Model to the Test Data ###
###
### Taking our test data and applying it to the RF model using the "predict" function from R's stat package.

pr <- predict(randomForestFit, Zoo_test)
pr

confusionMatrix(pr, reference = Zoo_test$type)

### Comparing Decision Boundaries of Popular Classification Techniques ###


```
